# -*- coding: utf-8 -*-
"""Oct_Schmallenberg_Ferrara_812.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GWamIiJRYFgkPnbecIn-h-f7lCA-POcE

# **SBV Southern Italy**

#         - - - - - - - - - - - - - - - - - - - - - - - - - - ----------   **PART ONE**   ---------- - - - - - - - - - - - - - - - - - - - - - - - - - -

# Step 1: Import Libraries
"""

# Importing necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix, roc_curve
import xgboost as xgb
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder

"""# Step 2: Load and Preprocess Data"""

from google.colab import files
uploaded = files.upload()

# Load the data (assuming it is an Excel file)
df = pd.read_excel('schmallenberg virus infection in southern Italy_812_Ferrara_2023.xlsx')

# Display the first few rows to verify the data
df.head(2)

# Drop irrelevant columns
df.drop(['ID ALL', 'ID ALL.1', 'PROVINCE'], axis=1, inplace=True)

"""ðŸ§§ **Re-Order**"""

# Reorder the columns in a more logical order
df = df[['AGE', 'AGE BINARY', 'ORIGIN', 'LOCATION', 'TYPE',
         'SPECIES', 'OUTCOME', 'Temperatura media',
         'cm Rainfall', 'altitude', 'Distance from the coast']]

"""# ðŸ§§: ðŸ§§: ðŸ§§: ðŸ§§: ðŸ§§: **Label Encoder** ðŸ§§: ðŸ§§: ðŸ§§: ðŸ§§: ðŸ§§:"""

# Convert categorical columns to numeric
# Encode categorical variables using LabelEncoder or mappings
# LabelEncoder
label_encoder = LabelEncoder()

# Encoding categorical variables
df['ORIGIN'] = label_encoder.fit_transform(df['ORIGIN'])
df['TYPE'] = label_encoder.fit_transform(df['TYPE'])
df['SPECIES'] = label_encoder.fit_transform(df['SPECIES'])
df['LOCATION'] = label_encoder.fit_transform(df['LOCATION'])

# Mapping
df['AGE BINARY'] = df['AGE BINARY'].map({'ADULT': 1, 'YOUNG': 0})
df['Temperatura media'] = df['Temperatura media'].map({'>16': 1, '<16': 0})
df['cm Rainfall'] = df['cm Rainfall'].map({'>1000': 1, '<1000': 0})
df['altitude'] = df['altitude'].map({'>250m': 1, '<250m': 0})
df['Distance from the coast'] = df['Distance from the coast'].map({'> 20': 1, '< 20': 0})

# Verify there are no more missing values
print(df.isnull().sum())

df.head(2)

"""# Step 3: Feature Selection and Data Splitting"""

# Feature selection: Drop the target column and select relevant features
X = df.drop('OUTCOME', axis=1)  # Features
y = df['OUTCOME']  # Target variable

# Normalize the features using StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)

"""# Step 4: Model Training"""

# Initialize models
tree_model = DecisionTreeClassifier(max_depth=5, criterion='entropy', random_state=42)
knn_model = KNeighborsClassifier(n_neighbors=3)
logistic_model = LogisticRegression(max_iter=1000, random_state=42)
xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Train models
tree_model.fit(X_train, y_train)
knn_model.fit(X_train, y_train)
logistic_model.fit(X_train, y_train)
# Train XGBoost without the deprecated parameter
xgb_model = xgb.XGBClassifier(eval_metric='logloss', random_state=42)
xgb_model.fit(X_train, y_train)
print("XGBoost model retrained without the deprecated parameter.")


print("All models trained successfully.")

"""# Step 5: Model Evaluation"""

# Define a function for model evaluation
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])
    print(f'Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}, AUC: {auc:.4f}')
    print(classification_report(y_test, y_pred))
    return accuracy, f1, auc

# Evaluate each model
print("Decision Tree Evaluation:")
evaluate_model(tree_model, X_test, y_test)

print("KNN Evaluation:")
evaluate_model(knn_model, X_test, y_test)

print("Logistic Regression Evaluation:")
evaluate_model(logistic_model, X_test, y_test)

print("XGBoost Evaluation:")
evaluate_model(xgb_model, X_test, y_test)

"""# Step 6: Cross-Validation"""

# Perform 10-fold cross-validation for each model
kf = KFold(n_splits=10, random_state=42, shuffle=True)

# Cross-validate each model
models = {
    'Decision Tree': tree_model,
    'KNN': knn_model,
    'Logistic Regression': logistic_model,
    'XGBoost': xgb_model
}

for name, model in models.items():
    cv_scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy')
    print(f'{name}: Cross-Validation Accuracy = {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}')

"""#         - - - - - - - - - - - - - - - - - - - - - - - - - - ----------   **PART TWO**   ---------- - - - - - - - - - - - - - - - - - - - - - - - - - -

# Step 7: Model Interpretability Using SHAP
"""

!pip install shap

import shap

# Create a SHAP explainer for XGBoost model
explainer = shap.TreeExplainer(xgb_model)
shap_values = explainer.shap_values(X_test)

# Summary plot for SHAP values
shap.summary_plot(shap_values, X_test, feature_names=X.columns)

"""# Step 8: Visualization of ROC Curve"""

# Plot ROC curve for XGBoost model
fpr, tpr, _ = roc_curve(y_test, xgb_model.predict_proba(X_test)[:, 1])
plt.plot(fpr, tpr, label=f'XGBoost (AUC = {roc_auc_score(y_test, xgb_model.predict_proba(X_test)[:, 1]):.4f})')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()